{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7560c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_languages = ['zu','sw','ki','kam','ig']\n",
    "source_language = \"en\"\n",
    "\n",
    "lc = False  # If True, lowercase the data.\n",
    "seed = 42  # Random seed for shuffling.\n",
    "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install opus-tools\n",
    "! pip install opustools-pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look into directories manenos here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c758c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_corpus(source_language, target_language):\n",
    "    os.environ[\"src\"] = source_language\n",
    "    os.environ[\"tgt\"] = target_language\n",
    "\n",
    "    os.environ[\"tag\"] = tag\n",
    "\n",
    "    os.system(\"opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\")\n",
    "\n",
    "    # extract the corpus file\n",
    "    os.system(\"gunzip JW300_latest_xml_$src-$tgt.xml.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dff594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_test_set(source_language, target_language):\n",
    "    #Download test set for english- kiswahili, to be used for maml validation\n",
    "    os.environ[\"trg\"] = target_language\n",
    "    os.environ[\"src\"] = source_language \n",
    "\n",
    "    os.system(\"wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en\")\n",
    "    os.system(\"mv test.en-$trg.en test.en\")\n",
    "    os.system(\"wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg\")\n",
    "    os.system(\"mv test.en-$trg.$trg test.$trg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2a5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make directories for all the 5 langs, where all the downloaded data will be placed\n",
    "#enzu,ensw,enki,enkam,enig\n",
    "\n",
    "for i in range(len(target_languages)):\n",
    "    target_language = target_languages[i]\n",
    "    data_path = source_language+target_language\n",
    "    os.system(\"mkdir %s\" % data_path)\n",
    "    \n",
    "    os.system(\"cd %s\" % data_path)\n",
    "    download_corpus(source_language, target_language)\n",
    "    download_test_set(source_language, target_language)\n",
    "    \n",
    "    os.system(\"cd ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23324aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the global test set.\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check your directory before\n",
    "def filter_test_data:\n",
    "    # Read the test data to filter from train and dev splits.\n",
    "    # Store english portion in set for quick filtering checks.\n",
    "    en_test_sents = set()\n",
    "    filter_test_sents = \"test.en-any.en\"\n",
    "    j = 0\n",
    "    with open(filter_test_sents) as f:\n",
    "      for line in f:\n",
    "        en_test_sents.add(line.strip())\n",
    "        j += 1\n",
    "    print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304495e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "172af742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_data_frame(source_file, target_file):\n",
    "    source = []\n",
    "    target = []\n",
    "    skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
    "    with open(source_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # Skip sentences that are contained in the test set.\n",
    "            if line.strip() not in en_test_sents:\n",
    "                source.append(line.strip())\n",
    "            else:\n",
    "                skip_lines.append(i)             \n",
    "    with open(target_file) as f:\n",
    "        for j, line in enumerate(f):\n",
    "            # Only add to corpus if corresponding source was not skipped.\n",
    "            if j not in skip_lines:\n",
    "                target.append(line.strip())\n",
    "\n",
    "    print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
    "\n",
    "    df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
    "    # if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
    "    #df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
    "    df.head(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39d81554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.7/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.7/site-packages (0.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from python-Levenshtein) (49.6.0.post20210108)\n"
     ]
    }
   ],
   "source": [
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "\n",
    "import time\n",
    "from fuzzywuzzy import process\n",
    "import numpy as np\n",
    "from os import cpu_count\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
    "# within a certain length of characters of the given sample.\n",
    "def fuzzfilter(sample, candidates, pad):\n",
    "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
    "  if len(candidates) > 0:\n",
    "    return process.extractOne(sample, candidates)[1]\n",
    "  else:\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def filter(df):\n",
    "    # drop duplicate translations\n",
    "    df_pp = df.drop_duplicates()\n",
    "\n",
    "    # drop conflicting translations\n",
    "    # (this is optional and something that you might want to comment out \n",
    "    # depending on the size of your corpus)\n",
    "    df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "    df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "    # Shuffle the data to remove bias in dev set selection.\n",
    "    df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    # reset the index of the training set after previous filtering\n",
    "    df_pp.reset_index(drop=False, inplace=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    ### iterating over pandas dataframe rows is not recomended, let use multi processing to apply the function\n",
    "\n",
    "    with Pool(cpu_count()-1) as pool:\n",
    "        scores = pool.map(partial(fuzzfilter, candidates=list(en_test_sents), pad=5), df_pp['source_sentence'])\n",
    "    hours, rem = divmod(time.time() - start_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"done in {}h:{}min:{}seconds\".format(hours, minutes, seconds))\n",
    "\n",
    "    # Filter out \"almost overlapping samples\"\n",
    "    df_pp = df_pp.assign(scores=scores)\n",
    "    df_pp = df_pp[df_pp['scores'] < 95]\n",
    "    \n",
    "    return df_pp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
    "# We use 1000 dev test and the given test set.\n",
    "import csv\n",
    "def save_files(df_pp, source_language, target_language):\n",
    "    # Do the split between dev/train and create parallel corpora\n",
    "    num_dev_patterns = 1000\n",
    "\n",
    "    # Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
    "    if lc:  # Julia: making lowercasing optional\n",
    "        df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
    "        df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
    "\n",
    "    # Julia: test sets are already generated\n",
    "    dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
    "    stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
    "\n",
    "\n",
    "    train_file = os.path.join(\"train.\")\n",
    "    dev_file = os.path.join(\"dev.\")\n",
    "\n",
    "    with open(train_file+source_language, \"w\") as src_file, open(train_file+target_language, \"w\") as trg_file:\n",
    "      for index, row in stripped.iterrows():\n",
    "        src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "        trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "\n",
    "    with open(dev_file+source_language, \"w\") as src_file, open(dev_file+target_language, \"w\") as trg_file:\n",
    "      for index, row in dev.iterrows():\n",
    "        src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "        trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "\n",
    "\n",
    "    # Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
    "    os.system(\"head train.*\")\n",
    "    os.system(\"head dev.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f06e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "def tokenize(source_language, target_language):\n",
    "    os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "    os.environ[\"tgt\"] = target_language\n",
    "\n",
    "    # Learn BPEs on the training data.\n",
    "    #os.environ[\"data_path\"] = path.join(\"..\",\"joeynmt\", \"data\", source_language + target_language) # Herman!\n",
    "\n",
    "    !pwd;ls\n",
    "\n",
    "    ! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
    "\n",
    "    # Apply BPE splits to the development and test data.\n",
    "    ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "    ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
    "\n",
    "    ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "    ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
    "    ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
    "    ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Create directory, move everyone we care about to the correct location\n",
    "# ! mkdir -p $data_path\n",
    "# ! cp train.* $data_path\n",
    "# ! cp test.* $data_path\n",
    "# ! cp dev.* $data_path\n",
    "# ! cp bpe.codes.4000 $data_path\n",
    "# ! ls $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### figure out how to call this properly\n",
    "\n",
    "#build vocab and print sample sents\n",
    "# ! sudo chmod 777 ../joeynmtmaml/scripts/build_vocab.py\n",
    "# ! ../joeynmtmaml/scripts/build_vocab.py ../joeynmt/data/$src$tgt/train.bpe.$src ../joeynmt/data/$src$tgt/train.bpe.$tgt --output_path ../joeynmt/data/$src$tgt/vocab.txt\n",
    "\n",
    "# # Some output\n",
    "# ! echo \"BPE Xhosa Sentences\"\n",
    "# ! tail -n 5 test.bpe.$tgt\n",
    "# ! echo \"Combined BPE Vocab\"\n",
    "# ! tail -n 10 ../joeynmt/data/$src$tgt/vocab.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3202ad6",
   "metadata": {},
   "source": [
    "### JoeyNMTMaml installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9415e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Freshia/joeynmtmaml.git\n",
    "!cd joeynmtmaml; pip3 install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52a921",
   "metadata": {},
   "source": [
    "#### Bringing it all together!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd to lang specific directory??\n",
    "def pre_process_data(source_language, target_language):\n",
    "    source_file = os.path.join('jw300.' + source_language)\n",
    "    target_file = os.path.join('jw300.' + target_language)\n",
    "    print(source_file)\n",
    "    print(target_file)\n",
    "    \n",
    "    df = create_data_frame(source_file, target_file)\n",
    "    df_pp = filter(df)\n",
    "    \n",
    "    \n",
    "    save_files(df_pp, source_language, target_language)\n",
    "    \n",
    "    tokenize(source_language, target_language)\n",
    "    \n",
    "    #make vocab file\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process data for all languages, and store in their respective directories\n",
    "for i in range(len(target_languages)):\n",
    "    target_language = target_languages[i]\n",
    "    data_path = source_language+target_language\n",
    "    os.system(\"cd %s\" % data_path)\n",
    "    \n",
    "    pre_process_data(source_language, target_language)\n",
    "    \n",
    "    os.system(\"cd ..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a6fba",
   "metadata": {},
   "source": [
    "### Torch and Learn2Learn Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==1.8.0 torchvision==0.9.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e524be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install learn2learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55d067",
   "metadata": {},
   "source": [
    "### Creating the JoeyNMT Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = 'langexp'\n",
    "#gdrive_path = os.environ[\"gdrive_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{name}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: [\"swen\",\"kien\",\"kamen\",\"igen\",\"zuen\"]\n",
    "    trg: [\"sw\",\"ki\",\"kam\",\"ig\",\"zu\"]\n",
    "    train: \"data/langdata/train.bpe\"\n",
    "    dev:   \"data/langdata/dev.bpe\"\n",
    "    test:  \"data/langdata/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    #src_vocab: \"data/{name}/vocab.txt\"\n",
    "    #trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0002\n",
    "    learning_rate_min: 0.00000001\n",
    "    maml_lr: 0.001\n",
    "    label_smoothing: 0.0\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    valid_batch_size: 3600       #for validation per task\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    iterations: 50\n",
    "    adaptation_steps: 5000\n",
    "    validation_freq: 5\n",
    "    valid_config: \"joeynmtmaml/configs/transformer_ensw_validation.yaml\"\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_transformer\"\n",
    "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\"\n",
    "with open(\"joeynmtmaml/configs/transformer_lang_maml_exp.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db4d93",
   "metadata": {},
   "source": [
    "### Creating the validation config for maml. Swahili language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"transformer_langexp_valid.yaml\"\n",
    "\n",
    "data:\n",
    "    src: \"swen\"\n",
    "    trg: \"sw\"\n",
    "    train: \"data/langdata/train.bpe\"\n",
    "    dev:   \"data/langdata/dev.bpe\"\n",
    "    test:  \"data/langdata/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"data/ensw/vocab.txt\"\n",
    "    trg_vocab: \"data/ensw/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/ensw_valid_transformer\"\n",
    "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\"\n",
    "with open(\"joeynmtmaml/configs/transformer_ensw_validation.yaml\",'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70823b65",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd joeynmt;python3 -m joeynmt train ../joeynmtmaml/configs/transformer_lang_maml_exp.yaml/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
